{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6_1 - Tim Demetriades\n",
    "10/8/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are the advantages of a CNN over a fully connected deep neural network for image classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of advantages to using a Convolutional Neural Network (CNN) over fully connected deep neural networks for image classification. Although fully connected deep neural networks can be used for many uses-cases including image classification, they are very general and not very good at feature extraction like CNNs. CNNs are trained to identify and extract the best features from images. CNNs are much more specialized and efficient at image classification. \n",
    "\n",
    "Fully connected deep neural networks use every node in the network. This is fine for small images but for very large images this can be very memory and process intensive. In CNNs, the convolution layer reduces the number of input nodes for the next layer. Furthermore, the pooling layers even further reduce the dimensionality. Instead of considering all of the pixels in the image, you are now only considering chunks of the image at a time.\n",
    "\n",
    "Another advatage is that CNNs can tolerate small shifts in where the pixels are in the image. The subject of the image may not always be in the same position and can even be rotated. More specifically, it can handle translation invariance. A normal fully connected deep neural network may not be able to classify this as the same object, but a CNN can.\n",
    "\n",
    "One final advantage of CNNs is that they can take advantage of the correlation that complex images may have. In many images you will have regions of the same color. Instead of considering all of the individual pixels, CNNs apply a filter which look at regions of pixels instead of just one pixel at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of using max pooling is that it can handle translation invariance, as mentioned in answer to the previous question. Pooling also reduces the number of nodes which can make the convolutions faster and more efficient.\n",
    "\n",
    "The point of using the convolution layers is to match identical features between different images. Although it will reduce the number of nodes, that is mainly the job of the max pooling layers. The max pooling layers reduce the size of the image which can decrease computational complexity and reduce noise. Therefore, both should be used for different reasons. The convolution layers should feed into the max pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. When would you want to add a local response normalization layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name implies, these layers are used to add normalization to the model. These normalization layers can add a sort of inhibition scheme. Inhibition schemes are similar to the concept of lateral inhibition in neurobiology. Basically, with lateral inhibition, the sensory perception is increased by having excited neurons subdue its neighbors, created a local maxima and a contrast in that area.\n",
    "\n",
    "This concept is implemented with local response normalization layers by normalizing the area around the local neighborhood of the excited neuron (node), making it become even more sensitive as compared to its neighbors. This allows you to detect high frequency features with a large response. Conversally, the responses that are uniformly large in any given local neighborhood will be dampened. The goal here is to boost the neurons with relatively large activations while dampening the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test below CNN codes with MNIST data set and show the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import needed modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic(u'matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load mnist data (digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train original shape (60000, 28, 28)\n",
      "y_train original shape (60000,)\n",
      "X_test original shape (10000, 28, 28)\n",
      "y_test original shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape)\n",
    "print(\"X_test original shape\", X_test.shape)\n",
    "print(\"y_test original shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show first digit and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class 5')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title('Class '+ str(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape x_train and x_test, convert to float, and divide by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train/=255\n",
    "X_test/=255\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert class vectors to binary class matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_classes = 10\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "\n",
    "y_train[0], Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are three steps to Convolution**\n",
    "1. Convolution\n",
    "2. Activation\n",
    "3. Pooling\n",
    "\n",
    "Repeat steps 1, 2, and 3 for adding more hidden layers. After that make a fully connected network. This fully connected network gives the ability to the CNN to classify the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()                                      # define sequential model\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))      # convolution\n",
    "model.add(Activation('relu'))                             # activation\n",
    "BatchNormalization(axis=-1)                               # batch normalization\n",
    "model.add(Conv2D(32, (3, 3)))                             # convolution\n",
    "model.add(Activation('relu'))                             # activation\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))                  # pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchNormalization(axis=-1)                               # batch normalization\n",
    "model.add(Conv2D(64,(3, 3)))                              # convolution\n",
    "model.add(Activation('relu'))                             # activation\n",
    "BatchNormalization(axis=-1)                               # batch normalization\n",
    "model.add(Conv2D(64, (3, 3)))                             # convolution\n",
    "model.add(Activation('relu'))                             # activation\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))                  # pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())                                      # flatten for fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "BatchNormalization()                                      # batch normalization\n",
    "model.add(Dense(512))                                     # dense layer\n",
    "model.add(Activation('relu'))                             # activation\n",
    "BatchNormalization()                                      # batch normalization\n",
    "model.add(Dropout(0.2))                                   # dropout\n",
    "model.add(Dense(10))                                      # dense layer (10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Convolution2D(10,3,3, border_mode='same'))\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 594,922\n",
      "Trainable params: 594,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches of tensor image data with real-time data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3, height_shift_range=0.08, zoom_range=0.08)\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take data and label arrays and generate batches of augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = gen.flow(X_train, Y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(X_test, Y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on data yielded batch-by-batch by a Python generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timjr\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "937/937 [==============================] - 47s 50ms/step - loss: 0.1991 - accuracy: 0.9369 - val_loss: 0.0317 - val_accuracy: 0.9904\n",
      "Epoch 2/5\n",
      "937/937 [==============================] - 46s 49ms/step - loss: 0.0623 - accuracy: 0.9806 - val_loss: 0.0526 - val_accuracy: 0.9840\n",
      "Epoch 3/5\n",
      "937/937 [==============================] - 49s 52ms/step - loss: 0.0474 - accuracy: 0.9853 - val_loss: 0.0248 - val_accuracy: 0.9920\n",
      "Epoch 4/5\n",
      "937/937 [==============================] - 45s 48ms/step - loss: 0.0400 - accuracy: 0.9877 - val_loss: 0.0190 - val_accuracy: 0.9931\n",
      "Epoch 5/5\n",
      "937/937 [==============================] - 45s 48ms/step - loss: 0.0341 - accuracy: 0.9898 - val_loss: 0.0190 - val_accuracy: 0.9938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x262f78fb310>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X_train, Y_train, batch_size=128, nb_epoch=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=5, validation_data=test_generator, validation_steps=10000//64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print model test accuracy using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0190 - accuracy: 0.9938\n",
      "\n",
      "Test accuracy:  0.9937999844551086\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print()\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions using model and test data. Output predictions to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "predictions = list(predictions)\n",
    "actuals = list(y_test)\n",
    "\n",
    "sub = pd.DataFrame({'Actual': actuals, 'Predictions': predictions})\n",
    "sub.to_csv('./output_cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model is a **CNN** made up of several hidden layers. The hidden layers involve convolution, activation, pooling, normalization, and fully connected layers. The data set used here is the mnist data set which comprises of 60,000 numeric digits from 0-9 in the training set and 10,000 in the testing set.\n",
    "\n",
    "After the data is prepared, the model is setup and trained using the training data and python generators. The training only uses 5 epochs, but this is more than sufficient. As can be seen from the results, the first epoch already has a **validation loss of only 0.0317 and validation accuracy of 99.04%**. By the 5th epoch, these values are down to **0.019 and 99.38** respectively.\n",
    "\n",
    "The model is evaluated on the test data and a test accuracy of 99.38% is achieved (the same as the validation accuracy at the end of training). \n",
    "\n",
    "As one can see, this CNN performs exceptionally well on this dataset which happens to be comprised of images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
